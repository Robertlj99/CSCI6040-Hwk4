{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zz-PFMXxN3fi"
   },
   "source": [
    "<center><h1>CSCI 4140: Natural Language Processing</h1></center>\n",
    "<center><h1>CSCI/DASC 6040: Computational Analysis of Natural Languages</h1></center>\n",
    "\n",
    "<center><h6>Spring 2023</h6></center>\n",
    "<center><h6>Homework 4 - N-gram and neural language models</h6></center>\n",
    "<center><h6>Due Sunday, March 26, at 11:59 PM</h6></center>\n",
    "\n",
    "<center><font color='red'>Do not redistribute without the instructor’s written permission.</font></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcbM1VSrN3fj"
   },
   "source": [
    "# Setup\n",
    "<font color='red'>Notes:\n",
    "\n",
    "- You must run the code for Q2 on a computer with GPU (running it on CPU will take much, much longer). [Google Colab](https://colab.research.google.com) is a good choice.\n",
    "    - If you're using Colab, make sure you upload the `wiki` files.\n",
    "    - If you're using other computer, update the path to `wiki` files (`fname = \"...`).\n",
    "- The neural language model may take up to 10 minutes to train, so **start early**!\n",
    "- The rest of the cells are designed so that you can run them in a few minutes of computation time. If it is taking longer than that, you probably have made a mistake in your code.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "X8bZDdek4mo2"
   },
   "outputs": [],
   "source": [
    "import torch, pickle, os, sys, random, time\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn, optim\n",
    "from collections import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OCiaiiPsN3fk"
   },
   "source": [
    "We'll start by loading the data. The WikiText language modeling dataset is a collection of tokens extracted from the set of verified Good and Featured articles on Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Y6IW47fRN3fl"
   },
   "outputs": [],
   "source": [
    "data = {'test': '', 'train': '', 'valid': ''}\n",
    "\n",
    "for data_split in data:\n",
    "    fname = \"./content/wiki.{}.tokens\".format(data_split)\n",
    "    with open(fname, encoding=\"utf8\") as f_wiki:\n",
    "        data[data_split] = f_wiki.read().lower().split()\n",
    "\n",
    "vocab = list(set(data['train']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzRJcts-xirF"
   },
   "source": [
    "Now have a look at the data by running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "GLE1v7mRIKt-",
    "outputId": "c15174a4-bd2d-4e6a-f763-de0e7ffb0329"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : ['=', 'valkyria', 'chronicles', 'iii', '=', 'senjō', 'no', 'valkyria', '3', ':'] ...\n",
      "3\n",
      "213886\n",
      "2051910\n",
      "dev : ['=', 'homarus', 'gammarus', '=', 'homarus', 'gammarus', ',', 'known', 'as', 'the'] ...\n",
      "test : ['=', 'robert', '<unk>', '=', 'robert', '<unk>', 'is', 'an', 'english', 'film'] ...\n",
      "first 10 words in vocab: ['kokanee', 'clothing', 'amino', 'damaged', 'modes', 'bhopali', 'mumia', 'demonstrates', 'zealand', 'assert']\n",
      "28911\n"
     ]
    }
   ],
   "source": [
    "print('train : %s ...' % data['train'][:10])\n",
    "print(len(data))\n",
    "print(len(data['valid']))\n",
    "print(len(data['train']))\n",
    "print('dev : %s ...' % data['valid'][:10])\n",
    "print('test : %s ...' % data['test'][:10])\n",
    "print('first 10 words in vocab: %s' % vocab[:10])\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel:\n",
    "    \n",
    "    def __init__(self, order):\n",
    "        self.order = order\n",
    "        self.model = {}\n",
    "    \n",
    "    def train_ngram_lm(self, data, order):\n",
    "        order -= 1\n",
    "        data = ['<S>'] * order + data #\n",
    "        lm = defaultdict(Counter)\n",
    "        for i in range(len(data) - order):\n",
    "\n",
    "            #concatenate list of words into string\n",
    "            words = ' '.join(data[i:i+order])\n",
    "\n",
    "            #While loop for back off\n",
    "            while words:\n",
    "\n",
    "                #Check if words have been seen before, if so update counter\n",
    "                if words in lm.keys():\n",
    "                    count = lm[words]\n",
    "                    count.update(Counter([data[i + order]]))\n",
    "\n",
    "                #If string has not been seen before create new dictionary entry for it\n",
    "                else:\n",
    "                    lm[words] = Counter([data[i + order]])\n",
    "\n",
    "                #Remove the first word from the string\n",
    "                x = words.split()\n",
    "                x.pop(0)\n",
    "                words = ' '.join(x)\n",
    "\n",
    "            #Perform same if-else for empty string\n",
    "            if words in lm.keys():\n",
    "                count = lm[words]\n",
    "                count.update(Counter([data[i + order]]))\n",
    "\n",
    "            else:\n",
    "                lm[words] = Counter([data[i + order]])\n",
    "\n",
    "        lm1 = {}\n",
    "        lm2 = {}\n",
    "        #Convert counts to probabilities\n",
    "        for x, y in lm.items():\n",
    "            i = sum(y.values())       \n",
    "            for k, v in y.items():\n",
    "                lm2[k] = v/i\n",
    "            lm1[x] = lm2\n",
    "            lm2 = {}\n",
    "\n",
    "        return lm1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3TZ91yDrl1Oq"
   },
   "source": [
    "# Q1. N-gram Language model (40pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhEuT7w5ClUg"
   },
   "source": [
    "\n",
    "## Q1.1: Train N-gram language model (15pts)\n",
    "\n",
    "Complete the following `train_ngram_lm` function based on the following input/output specifications. If you've done it right, you should pass the tests in the cell below.\n",
    "\n",
    "*Input:*\n",
    "+ **data**: the data object created in the cell above that holds the tokenized Wikitext data\n",
    "+ **order**: the order of the model (i.e., the \"n\" in \"n-gram\" model). If order=3, we compute $p(w_2 | w_0, w_1)$.\n",
    "\n",
    "*Output:*\n",
    "+ **lm**: A dictionary where the key is the history and the value is a probability distribution over the next word computed using the maximum likelihood estimate from the training data. Importantly, this dictionary should include *backoff* probabilities as well; e.g., for order=4, we want to store $p(w_3 | w_0,w_1,w_2)$ as well as $p(w_3|w_1,w_2)$ and $p(w_3|w_2)$. \n",
    "\n",
    "Each key should be a single string where the words that form the history have been concatenated using spaces. Given a key, its corresponding value should be a dictionary where each word type in the vocabulary is associated with its probability of appearing after the key. For example, the entry for the history 'w1 w2' should look like:\n",
    "\n",
    "    \n",
    "    lm['w1 w2'] = {'w0': 0.001, 'w1' : 1e-6, 'w2' : 1e-6, 'w3': 0.003, ...}\n",
    "    \n",
    "In this example, we also want to store `lm['w2']` and `lm['']`, which contain the bigram and unigram distributions respectively.\n",
    "\n",
    "*Hint:* You might find the **defaultdict** and **Counter** classes in the **collections** module to be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Working model, passes all tests \n",
    "def train_ngram_lm(data, order=3):\n",
    "    \"\"\"\n",
    "        Train n-gram language model\n",
    "    \"\"\"\n",
    "    \n",
    "    order -= 1\n",
    "    data = ['<S>'] * order + data #\n",
    "    lm = defaultdict(Counter)\n",
    "    for i in range(len(data) - order):\n",
    "        \n",
    "        #concatenate list of words into string\n",
    "        words = ' '.join(data[i:i+order])\n",
    "\n",
    "        #While loop for back off\n",
    "        while words:\n",
    "\n",
    "            #Check if words have been seen before, if so update counter\n",
    "            if words in lm.keys():\n",
    "                count = lm[words]\n",
    "                count.update(Counter([data[i + order]]))\n",
    "\n",
    "            #If string has not been seen before create new dictionary entry for it\n",
    "            else:\n",
    "                lm[words] = Counter([data[i + order]])\n",
    "\n",
    "            #Remove the first word from the string\n",
    "            x = words.split()\n",
    "            x.pop(0)\n",
    "            words = ' '.join(x)\n",
    "\n",
    "        #Perform same if-else for empty string\n",
    "        if words in lm.keys():\n",
    "            count = lm[words]\n",
    "            count.update(Counter([data[i + order]]))\n",
    "\n",
    "        else:\n",
    "            lm[words] = Counter([data[i + order]])\n",
    "    \n",
    "    lm1 = {}\n",
    "    lm2 = {}\n",
    "    #Convert counts to probabilities\n",
    "    for x, y in lm.items():\n",
    "        i = sum(y.values())       \n",
    "        for k, v in y.items():\n",
    "            lm2[k] = v/i\n",
    "        lm1[x] = lm2\n",
    "        lm2 = {}\n",
    "    \n",
    "    return lm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "28913\n",
      "613754\n",
      "1990204\n"
     ]
    }
   ],
   "source": [
    "lm1 = train_ngram_lm(data['train'], order = 1)\n",
    "print(len(lm1))\n",
    "lm2 = train_ngram_lm(data['train'], order = 2)\n",
    "print(len(lm2))\n",
    "lm3 = train_ngram_lm(data['train'], order = 3)\n",
    "print(len(lm3))\n",
    "lm4 = train_ngram_lm(data['train'], order = 4)\n",
    "print(len(lm4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "QK0QYLxd49x3",
    "outputId": "3913e2bf-50bd-45a3-fcdc-19bbc84e6d8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking empty history ...\n",
      "checking probability distributions ...\n",
      "checking lengths of histories ...\n",
      "checking word distribution values ...\n",
      "Congratulations, you passed the ngram check!\n"
     ]
    }
   ],
   "source": [
    "def test_ngram_lm():\n",
    "  \n",
    "    print('checking empty history ...')\n",
    "    lm1 = train_ngram_lm(data['train'], order=1)\n",
    "    assert '' in lm1, \"empty history should be in the language model!\"\n",
    "    \n",
    "    print('checking probability distributions ...')\n",
    "    lm2 = train_ngram_lm(data['train'], order=2)\n",
    "    sample = [sum(lm2[k].values()) for k in random.sample(list(lm2), 10)]\n",
    "    assert all([a > 0.999 and a < 1.001 for a in sample]), \"lm[history][word] should sum to 1!\"\n",
    "    \n",
    "    print('checking lengths of histories ...')\n",
    "    lm3 = train_ngram_lm(data['train'], order=3)\n",
    "    assert len(set([len(k.split()) for k in list(lm3)])) == 3, \"lm object should store histories of all sizes!\"\n",
    "    \n",
    "    print('checking word distribution values ...')\n",
    "    assert lm1['']['the'] < 0.064 and lm1['']['the'] > 0.062 and \\\n",
    "           lm2['the']['first'] < 0.017 and lm2['the']['first'] > 0.016 and \\\n",
    "           lm3['the first']['time'] < 0.106 and lm3['the first']['time'] > 0.105, \\\n",
    "           \"values do not match!\"\n",
    "    \n",
    "    print(\"Congratulations, you passed the ngram check!\")\n",
    "    \n",
    "test_ngram_lm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EjKT7pwJE6WY"
   },
   "source": [
    "## Q1.2: Generate text from n-gram language model (10pts)\n",
    "\n",
    "Complete the following `generate_text` function based on these input/output requirements:\n",
    "\n",
    "*Input:*\n",
    "\n",
    "+ **lm**: the lm object is the dictionary you return from  the **train_ngram_lm** function\n",
    "+ **vocab**: vocab is a list of unique word types in the training set, already computed for you during data loading.\n",
    "+ **context**: the input context string that you want to condition your language model on, should be a space-separated string of tokens\n",
    "+ **order**: order of your language model (i.e., \"n\" in the \"n-gram\" model)\n",
    "+ **num_tok**: number of tokens to be generated following the input context\n",
    "\n",
    "\n",
    "*Output:*\n",
    "\n",
    "+ generated text, should be a space-separated string\n",
    "    \n",
    "*Hint:*\n",
    "\n",
    "After getting the next-word distribution given history, try using **[numpy.random.choice](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html)** to sample the next word from the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jx0BFoF1E5dF"
   },
   "outputs": [],
   "source": [
    "# generate text\n",
    "def generate_text(lm, vocab, context=\"he is the\", order=3, num_tok=25):\n",
    "    \n",
    "    # The goal is to generate new words following the context\n",
    "    # If context has more tokens than the order of lm, \n",
    "    # generate text that follows the last (order-1) tokens of the context\n",
    "    # and store it in the variable `history`\n",
    "    order -= 1\n",
    "    history = context.split()[-order:]\n",
    "    # `out` is the list of tokens of context\n",
    "    # you need to append the generated tokens to this list\n",
    "    out = context.split()\n",
    "    \n",
    "    #Add two characters not in vocab but in search keys to the vocab then check\n",
    "    #that specified search context is in vocab\n",
    "    spec_char = ['', '<S>']\n",
    "    spec_vocab = vocab + spec_char\n",
    "    if not all(item in spec_vocab for item in out):\n",
    "        print(\"Specified context not in vocab cannot be searched\")\n",
    "        \n",
    "        \n",
    "    for i in range(num_tok):\n",
    "        \"\"\"\n",
    "        IMPLEMENT ME!\n",
    "        \"\"\"\n",
    "        if order == 0:\n",
    "            history  = ['']\n",
    "        search = ' '.join(history)\n",
    "        dist = lm[search]\n",
    "        words = [k for k in dist.keys()]\n",
    "        probs = [v for v in dist.values()]\n",
    "        next_word = np.random.choice(words, size = 1, p = probs)\n",
    "        x = ' '.join(next_word)\n",
    "        out.append(x)\n",
    "        history.append(x)\n",
    "        history.pop(0)\n",
    "    \n",
    "    final_string = ' '.join(out)\n",
    "    print(final_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qPPhLK3HF5L"
   },
   "source": [
    "Now try to generate some texts! Read the texts generated by ngram language model with different orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BSlNevanHIlM",
    "outputId": "0069e46d-6b9e-4f55-9135-a4718d751f67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he is the styles the october , and , new had publication to . of , special crystal borough 5 may the stout <unk> becoming as sangatya had\n"
     ]
    }
   ],
   "source": [
    "order = 1\n",
    "generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Ibmvkwl9HMzd",
    "outputId": "eb364d37-42ed-4d4f-b821-f3d4c5676266"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he is the early thirties he joined the 18th and the target multiple places may have a statue in these unitary authorities in 1884 , albeit eating for\n"
     ]
    }
   ],
   "source": [
    "order = 2\n",
    "generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "BQNB3FqKHibm",
    "outputId": "aaf46a26-5776-419f-e3a9-f2700c65a623"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he is the sixteenth @-@ century inscription on a salary of hockey operations colin campbell stated that he would play veeru if that involves damage to infrastructure =\n"
     ]
    }
   ],
   "source": [
    "order = 3\n",
    "generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "eSO4l5z7HjGU",
    "outputId": "7b33d144-1b25-4d96-c338-4df0add847d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he is the lost baby , the elder and his son , kenton , refuted these claims in their correspondence the older man 's devotion was <unk> in\n"
     ]
    }
   ],
   "source": [
    "order = 4\n",
    "generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLovkCIrHy0H"
   },
   "source": [
    "## Q1.3 : Evaluate the models (15pts)\n",
    "Now let's evaluate the models quantitively using the intrinsic metric **perplexity**. \n",
    "\n",
    "Recall perplexity is the inverse probability of the test text\n",
    "$$\\text{PP}(w_1, \\dots, w_t) = P(w_1, \\dots, w_t)^{-\\frac{1}{T}}$$\n",
    "\n",
    "For an n-gram model, perplexity is computed by\n",
    "$$\\text{PP}(w_1, \\dots, w_t) = \\left[\\prod_{t=1}^T P(w_t|w_{t-1},\\ldots,w_{t-n+1})\\right]^{-\\frac{1}{T}}$$\n",
    "\n",
    "To address the numerical issue (underflow), we usually compute\n",
    "$$\\text{PP}(w_1, \\dots, w_t) = \\exp\\left(-\\frac{1}{T}\\sum_i \\log P(w_t|w_{t-1},\\ldots,w_{t-n+1})\\right)$$\n",
    "\n",
    "\n",
    "*Input:*\n",
    "\n",
    "+ **lm**: the language model you trained (the object you returned from the `train_ngram_lm` function)\n",
    "+ **data**: test data\n",
    "+ **vocab**: the list of unique word types in the training set\n",
    "+ **order**: order of the lm\n",
    "\n",
    "*Output:*\n",
    "\n",
    "+ the perplexity of test data\n",
    "\n",
    "*Hint:*\n",
    "\n",
    "+ If the history is not in the **lm** object, back-off to (n-1) order history to check if it is in **lm**. If no history can be found, just use `1/|V|` where `|V|` is the size of vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "FKi4AczgHj1t"
   },
   "outputs": [],
   "source": [
    "from math import log, exp\n",
    "def compute_perplexity(lm, data, vocab, order=3):\n",
    "    \n",
    "    # pad according to order\n",
    "    order -= 1\n",
    "    data = ['<S>'] * order + data\n",
    "    log_sum = 0\n",
    "    for i in range(len(data) - order):\n",
    "        h, w = ' '.join(data[i: i+order]), data[i+order]\n",
    "        \"\"\"\n",
    "        IMPLEMENT ME!\n",
    "        # if h not in lm, back-off to n-1 gram and look up again\n",
    "        \"\"\"\n",
    "        \n",
    "        x=1\n",
    "        while h not in lm.keys():\n",
    "            h = ' '.join(data[i+x:i+order])\n",
    "            x += 1\n",
    "        \n",
    "        while w not in lm[h].keys() and x <= order :\n",
    "            h = ' '.join(data[i+x:i+order])\n",
    "            x += 1\n",
    "                \n",
    "        if w not in lm[h].keys():\n",
    "            logx = log(1/len(v))\n",
    "            log_sum += logx\n",
    "            print('exception: ' + w)\n",
    "          \n",
    "        else:\n",
    "            logx = log(lm[h][w])\n",
    "            log_sum += logx\n",
    "        \n",
    "    normal_sum = (-1/len(data)) * log_sum\n",
    "    exp_sum = exp(normal_sum)\n",
    "    return exp_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVpItwZhI6ac"
   },
   "source": [
    "Let's evaluate the language model with different orders. You should see a decrease in perplexity as the order increases. As a reference, the perplexity of the unigram, bigram, trigram, and 4-gram language models should be around 795, 203, 141, and 130 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "SpN70HA2H9C-",
    "outputId": "83a830db-0c66-4ffe-a7e6-d2587c7f6b8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order 1 ppl 794.5377104541699\n",
      "order 2 ppl 203.26044811248715\n",
      "order 3 ppl 141.21882352172221\n",
      "order 4 ppl 129.63403564699973\n"
     ]
    }
   ],
   "source": [
    "for o in [1, 2, 3, 4]:\n",
    "    lm = train_ngram_lm(data['train'], order=o)\n",
    "    print('order {} ppl {}'.format(o, compute_perplexity(lm, data['test'], vocab, order=o)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9tRCKZ5BJJOV"
   },
   "source": [
    "# Q2. Neural language models (70pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jlXIjm6WZBE"
   },
   "source": [
    "In this part of the homework, we'll be using PyTorch to play around with neural language models. First, a quick warm up by implementing backpropagation within a *scalar* neural network. Then, you'll implement a neural language model using PyTorch's built-in modules.\n",
    "\n",
    "Firstly, run the cell below to import pytorch and set up the gradient checking functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VOhQHHAPV6LD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# checks equality between your gradients and those from autograd\n",
    "def gradient_check(params, your_gradient):\n",
    "    all_good = True\n",
    "    for key in params.keys():\n",
    "        if params[key].grad.size() != your_gradient[key].size():\n",
    "            print('GRADIENT ERROR for parameter %s, SIZE ERROR\\nyour size: %s\\nactual size: %s\\n'\\\n",
    "                % (key, your_gradient[key].size(), \n",
    "                   params[key].grad.size()))\n",
    "            all_good = False\n",
    "        elif not torch.allclose(params[key].grad, your_gradient[key], atol=1e-6):\n",
    "            print('GRADIENT ERROR for parameter %s, VALUE ERROR\\nyours: %s\\nactual: %s\\n'\\\n",
    "                % (key, your_gradient[key].detach(), \n",
    "                   params[key].grad))\n",
    "            all_good = False\n",
    "            \n",
    "    return all_good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kt0UNgpWuJ6"
   },
   "source": [
    "## Q2.1 Warm up with single neuron (10 pts)\n",
    "The following code cell trains a network with scalars (i.e., single neurons) in each layer on a small dataset of ten examples. All you have to do is translate the partial derivatives we computed into code. The network is defined as:\n",
    "\n",
    "<center>$\\text{h} = \\tanh(w_1 \\cdot \\text{input})$</center>\n",
    "\n",
    "<center>$\\text{pred} = \\tanh(w_2 \\cdot \\text{h})$</center>\n",
    "\n",
    "<center>$\\text{L} = 0.5 \\cdot (\\text{target} - \\text{pred})^2$</center>\n",
    "\n",
    "If you run the cell below, you should see \"GRADIENT ERRORS\". Once you implement the partial derivatives $\\frac{\\partial{L}}{\\partial{w_1}}$ and $\\frac{\\partial{L}}{\\partial{w_2}}$ correctly, you will instead see a \"SUCCESS\" message. **Do NOT modify any code outside of the block marked \"IMPLEMENT BACKPROP HERE\"!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HfLgwTNYWGt3",
    "outputId": "4f64e8ad-7366-4ad1-b280-f672fe4f8c27"
   },
   "outputs": [],
   "source": [
    "# initialize model parameters\n",
    "params = {}\n",
    "params['w1'] = torch.randn(1, 1, requires_grad=True) # input > hidden with scalar weight w1\n",
    "params['w2'] = torch.randn(1, 1, requires_grad=True) # hidden > output with scalar weight w2\n",
    "\n",
    "# set up some training data\n",
    "inputs = torch.randn(20, 1)\n",
    "targets = inputs / 2\n",
    "\n",
    "# training loop\n",
    "all_good = True\n",
    "for i in range(len(inputs)):\n",
    "    \n",
    "    ## forward prop, then compute loss.\n",
    "    a = params['w1'] * inputs[i] # intermediate variable, following lecture notes\n",
    "    hidden = torch.tanh(a)\n",
    "    b = params['w2'] * hidden\n",
    "    pred = torch.tanh(b)\n",
    "    loss = 0.5 * (targets[i] - pred) ** 2 # compute square loss\n",
    "    loss.backward() # runs autograd\n",
    "    \n",
    "    \n",
    "    ####################\n",
    "    # TODO: IMPLEMENT BACKPROP HERE\n",
    "    # DO NOT MODIFY ANY CODE OUTSIDE OF THIS BLOCK!!!!\n",
    "    your_gradient = {}\n",
    "    your_gradient['w1'] = torch.zeros(params['w1'].size()) # implement dL/dw1\n",
    "    your_gradient['w2'] = torch.zeros(params['w2'].size()) # implement dL/dw2\n",
    "    \n",
    "    # IMEPLEMENT ME!\n",
    "\n",
    "    # END \n",
    "    ####################\n",
    "    \n",
    "    if not gradient_check(params, your_gradient):\n",
    "        all_good = False\n",
    "        break\n",
    "    \n",
    "    # zero gradients after each training example\n",
    "    params['w1'].grad.zero_()\n",
    "    params['w2'].grad.zero_() \n",
    "    \n",
    "if all_good:\n",
    "    print('SUCCESS! you passed the gradient check.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5bzxVSAOPUn"
   },
   "source": [
    "## Q2.2 RNN language model (20 pts)\n",
    "\n",
    "For this part of the homework, we will use **PyTorch** to build our model. The following code cell preprocesses the raw text so you can load it directly. The input to your model is a *minibatch* of sequences which takes the form of a  $N \\times L$ matrix  where $N$ is the batch size and $L$ is the maximum sequence length. For each minibatch, your models should produce an $N \\times L \\times V$ tensor where $V$ is the size of the vocabulary. This tensor stores the predicted probability distribution of the next word for every position of every sequence in the batch. Note that each batch is padded to dimensionality $L=40$ using the special padding token <*pad>*; similarly, each sequence begins with the <*bos>* token and ends with the <*eos>* token. Please look at the [PyTorch RNN documentation](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) if you're having problems getting started.\n",
    "\n",
    "First, run the following code cell to download the data.\n",
    "\n",
    "<font color=\"red\">Please change your Colab runtime to the GPU backend by going to \"Runtime > Change runtime type > Hardware accelerator > GPU\".</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xtOZWDtTSCAG",
    "outputId": "245ce512-43d5-4bef-ef4f-adc59bab5ce7"
   },
   "outputs": [],
   "source": [
    "import torch, pickle, os, sys, random, time\n",
    "from torch import nn, optim\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device: ', device)\n",
    "\n",
    "# Load id2word from wikitext pickle\n",
    "with open('wikitext.pkl', 'rb') as f_in:\n",
    "    wikitext = pickle.load(f_in)\n",
    "\n",
    "wikitext['train'] = torch.LongTensor(wikitext['train']).to(device)\n",
    "wikitext['dev'] = torch.LongTensor(wikitext['valid']).to(device)\n",
    "wikitext['test'] = torch.LongTensor(wikitext['test']).to(device)\n",
    "idx_to_word = wikitext['id2word']\n",
    "word_to_idx = {idx_to_word[k]: k for k in idx_to_word}\n",
    "\n",
    "print(\"Wikitext data loaded!\")\n",
    "# Demonstrate id2word\n",
    "print('There are ' + str(len(idx_to_word)) + ' words in vocabulary')\n",
    "for id in range(8):\n",
    "    print('Word id ' + str(id) + \" stands for '\" + str(idx_to_word[id]) + \"\\'\")\n",
    "print('...')\n",
    "print((wikitext['train'] > 0).sum())\n",
    "    \n",
    "print('Set up finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_3C-13G_m2Q"
   },
   "source": [
    "The following cell contains code for computing perplexity and training the neural language model. Run the cell, and please make sure you (at least roughly) understand what is happening, but **do not modify any part of it**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "onfjIrblON0g"
   },
   "outputs": [],
   "source": [
    "# function to evaluate LM perplexity on some input data, DO NOT MODIFY\n",
    "def compute_perplexity(dataset, net, bsz=64):\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n",
    "    num_examples, seq_len = dataset.size()\n",
    "    \n",
    "    # we'll still use batches because we can't fit the whole\n",
    "    # validation set into GPU memory\n",
    "    batches = [(start, start + bsz) for start in range(0, num_examples, bsz)]\n",
    "    \n",
    "    total_unmasked_tokens = 0. # count how many unpadded tokens there are\n",
    "    nll = 0.\n",
    "    for b_idx, (start, end) in enumerate(batches):\n",
    "        batch = dataset[start:end]\n",
    "        ut = torch.nonzero(batch).size(0)\n",
    "        preds = net(batch)\n",
    "        targets = batch[:, 1:].contiguous().view(-1)\n",
    "        preds = preds[:, :-1, :].contiguous().view(-1, net.vocab_size)\n",
    "        loss = criterion(preds, targets)\n",
    "        nll += loss.detach()\n",
    "        total_unmasked_tokens += ut\n",
    "\n",
    "    perplexity = torch.exp(nll / total_unmasked_tokens).cpu()\n",
    "    return perplexity.data\n",
    "    \n",
    "\n",
    "# training loop for language models, DO NOT MODIFY!\n",
    "def train_lm(dataset, params, net):\n",
    "    \n",
    "    # since the first index corresponds to the PAD token, we just ignore it\n",
    "    # when computing the loss\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    \n",
    "    optimizer = optim.Adam(net.parameters(), lr=params['learning_rate'])\n",
    "    num_examples, seq_len = dataset.size()    \n",
    "    batches = [(start, start + params['batch_size']) for start in\\\n",
    "               range(0, num_examples, params['batch_size'])]\n",
    "    \n",
    "    for epoch in range(params['epochs']):\n",
    "        ep_loss = 0.\n",
    "        start_time = time.time()\n",
    "        random.shuffle(batches)\n",
    "        net.train()\n",
    "        # for each batch, calculate loss and optimize model parameters            \n",
    "        for b_idx, (start, end) in enumerate(batches):\n",
    "            batch = dataset[start:end]\n",
    "            preds = net(batch)\n",
    "\n",
    "            preds = preds[:, :-1, :].contiguous().view(-1, net.vocab_size)\n",
    "            targets = batch[:, 1:].contiguous().view(-1)\n",
    "            loss = criterion(preds, targets)\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(net.parameters(), 3)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            ep_loss += loss\n",
    "        \n",
    "        net.eval()\n",
    "        print('epoch: %d, loss: %0.2f, time: %0.2f sec, dev perplexity: %0.2f' %\\\n",
    "              (epoch, ep_loss, time.time()-start_time, compute_perplexity(wikitext['dev'], net)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVwFC8cHLWye"
   },
   "source": [
    "Now implement the following class, which defines a recurrent neural language model, by implementing the `forward` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IBC5pTTgIBVH"
   },
   "outputs": [],
   "source": [
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(RNNLM, self).__init__()\n",
    "        self.vocab_size = params['vocab_size']\n",
    "        self.d_emb = params['d_emb']  # size of word-embedding vector\n",
    "        self.d_hid = params['d_hid']  # vector size of the hidden layer\n",
    "        self.n_layer = 1\n",
    "        self.batch_size = params['batch_size']\n",
    "        \n",
    "        self.encoder = nn.Embedding(self.vocab_size, self.d_emb)\n",
    "        self.rnn = nn.RNN(self.d_emb, self.d_hid, self.n_layer, batch_first=True)\n",
    "        self.decoder = nn.Linear(self.d_hid, self.vocab_size)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        \"\"\"\n",
    "            IMPLEMENT ME!\n",
    "            Encode the data using the embedding layer you initialized.\n",
    "            Pass the encoded data and hidden states to your RNN.\n",
    "            Return unnormalized logits for each token's prediction.\n",
    "            \n",
    "            Why just logits? Check the document of torch.nn.CrossEntropyLoss,\n",
    "            since it combines nn.LogSoftmax() and nn.NLLLoss(), \n",
    "            you don't need to explicitly use the softmax function!\n",
    "        \"\"\"\n",
    "        batch_size, seq_len= batch.shape\n",
    "        hidden = (torch.zeros(self.n_layer, batch_size, self.d_hid).to(device))\n",
    "\n",
    "        pass        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfzBU3SCWIso"
   },
   "source": [
    "Run the following cell to test that your implementation is at least returning tensors of the proper dimensionality. Note that this is just a sanity check. Your `RNNLM` might still be implemented incorrectly even if it passes. You will have to obtain a reasonable perplexity after training on WikiText to be certain that you've done it right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wv3NEVi7AaCE",
    "outputId": "83529cfc-a3d7-46f6-fd1b-e5573c500cc4"
   },
   "outputs": [],
   "source": [
    "def test_RNNLM():\n",
    "    test_batch = torch.LongTensor(5, 4).random_(0, 10).to(device)\n",
    "    params = {}\n",
    "    params['vocab_size'] = len(idx_to_word)\n",
    "    params['d_emb'] = 8\n",
    "    params['d_hid'] = 8\n",
    "    params['batch_size'] = 5\n",
    "    testnet = RNNLM(params)\n",
    "    testnet.to(device)\n",
    "    test_output = testnet(test_batch)\n",
    "    assert test_output.shape[0] == params['batch_size'], \"size of dimension 0 is incorrect, expect %i but got %i\" % \\\n",
    "                                                          (params['batch_size'], test_output.shape[0])\n",
    "    assert test_output.shape[1] == test_batch.shape[1], \"size of dimension 1 is incorrect, expect %i but got %i\" % \\\n",
    "                                                          (test_batch.shape[1], test_output.shape[1])\n",
    "    assert test_output.shape[2] == params['vocab_size'], \"size of dimension 2 is incorrect, expect %i but got %i\" % \\\n",
    "                                                          (params['vocab_size'], test_output.shape[2])\n",
    "    print(\"Congratulations, you passed the RNNLM test!\")\n",
    "test_RNNLM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7li14NHkLimS"
   },
   "source": [
    "Once you pass the above test, train your `RNNLM` model on WikiText by running the cell below. It should take a couple minutes per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tGvKkmqMET1w",
    "outputId": "704e3d8d-b09a-4c65-fddf-8fc04e90e2fc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THESE HYPERPARAMETERS, WE WILL CHECK!\n",
    "params = {}\n",
    "params['vocab_size'] = len(idx_to_word)\n",
    "params['d_emb'] = 512\n",
    "params['d_hid'] = 256\n",
    "params['batch_size'] = 64\n",
    "params['epochs'] = 5\n",
    "params['learning_rate'] = 0.001\n",
    "\n",
    "RNNnet = RNNLM(params)\n",
    "RNNnet.to(device)\n",
    "train_lm(wikitext['train'], params, RNNnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwNvc39bLnqY"
   },
   "source": [
    "After training is finished, run the cell below to get the perplexity on the test set. If you did it right, your perplexity should be around 135-140."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xD6NYw62St2J",
    "outputId": "2c8c3113-92a8-4213-b819-c06367b4cd98"
   },
   "outputs": [],
   "source": [
    "RNNnet.eval() # we're no longer training the network\n",
    "print('%s perplexity: %0.2f' % ('test', compute_perplexity(wikitext['test'], RNNnet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKu_fZJ6VUL8"
   },
   "source": [
    "## Q2.3 Neural Language Model with attention (30 pts)\n",
    "\n",
    "Only start working at this after you've correctly implemented the `RNNLM` in the previous problem, as you'll want to copy over some code here. \n",
    "Complete the foward function of both the `ATTNLM` and `Attention` modules by following the instructions in the comment block. **Each epoch may take 3-5 minutes to run, so start early!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ja339lSzVajG"
   },
   "outputs": [],
   "source": [
    "# An RNN language model with attention, you implement this!\n",
    "class ATTNLM(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(ATTNLM, self).__init__()\n",
    "        \n",
    "        self.vocab_size = params['vocab_size']\n",
    "        self.d_emb = params['d_emb']\n",
    "        self.d_hid = params['d_hid']\n",
    "        self.n_layer = 1\n",
    "        self.btz = params['batch_size']\n",
    "        \n",
    "        self.encoder = nn.Embedding(self.vocab_size, self.d_emb)\n",
    "        self.attn = Attention(self.d_hid)\n",
    "        self.rnn = nn.RNN(self.d_emb, self.d_hid, self.n_layer, batch_first=True)\n",
    "        # the combined_W maps the combined hidden states and context vectors to d_hid \n",
    "        self.combined_W = nn.Linear(self.d_hid * 2, self.d_hid)\n",
    "        self.decoder = nn.Linear(self.d_hid, self.vocab_size)\n",
    "        \n",
    "\n",
    "    def forward(self, batch, return_attn_weights=False):\n",
    "        \n",
    "        \"\"\"\n",
    "            IMPLEMENT ME!\n",
    "            Copy your implementation of RNNLM, make sure it passes the RNNLM check\n",
    "            In addition to that, you need to add the following 3 things\n",
    "            1. pass rnn output to attention module, get context vectors and attention weights\n",
    "            2. concatenate the context vec and rnn output, pass the combined\n",
    "               vector to the layer dealing with the combined vectors (self.combined_W)\n",
    "            3. if return_attn_weights, instead of return the [N, L, V]\n",
    "               matrix, return the attention weight matrix\n",
    "               of dimension [N, L, L] which returned from the forrward function of Attention module\n",
    "        \"\"\"\n",
    "        batch_size, seq_len= batch.shape\n",
    "        hidden = torch.zeros(self.n_layer, batch_size, self.d_hid).to(device)\n",
    "                \n",
    "        pass\n",
    "        \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_hidden):\n",
    "        super(Attention, self).__init__()\n",
    "        self.linear_w1 = nn.Linear(d_hidden, d_hidden)\n",
    "        self.linear_w2 = nn.Linear(d_hidden, 1)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "      \n",
    "        \"\"\"\n",
    "            IMPLEMENT ME!\n",
    "            For each time step t\n",
    "                1. Obtain attention scores for step 0 to (t-1)\n",
    "                   This should be a dot product between current hidden state (x[:,t:t+1,:])\n",
    "                   and all previous states x[:, :t, :]. While t=0, since there is not\n",
    "                   previous context, the context vector and attention weights should be of zeros.\n",
    "                   You might find torch.bmm useful for computing over the whole batch.\n",
    "                2. Turn the scores you get for 0 to (t-1) steps to a distribution.\n",
    "                   You might find F.softmax to be helpful.\n",
    "                3. Obtain the sum of hidden states weighted by the attention distribution\n",
    "            Concat the context vector you get in step 3. to a matrix.\n",
    "            \n",
    "            Also remember to store the attention weights, the attention matrix \n",
    "            for each training instance should be a lower triangular matrix. Specifically,\n",
    "            each row, element 0 to t-1 should sum to 1, the rest should be padded with 0.\n",
    "            e.g. \n",
    "            [ [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "              [1.0000, 0.0000, 0.0000, 0.0000],\n",
    "              [0.4246, 0.5754, 0.0000, 0.0000],\n",
    "              [0.2798, 0.3792, 0.3409, 0.0000] ]\n",
    "            \n",
    "            Return the context vector matrix and the attention weight matrix\n",
    "            \n",
    "        \"\"\"\n",
    "        batch_seq_len = x.shape[1]\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqyjOwGz8Y6b"
   },
   "source": [
    "Run the following cell to sanity check your implementation; do not continue until you pass all of the tests!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_0-MHHC2KYv8",
    "outputId": "8a543a58-58a9-4847-8264-f61d1245b8de"
   },
   "outputs": [],
   "source": [
    "def test_ATTNLM():\n",
    "    test_batch = torch.LongTensor(5, 4).random_(0, 10).to(device)\n",
    "    params = {}\n",
    "    params['vocab_size'] = len(idx_to_word)\n",
    "    params['d_emb'] = 8\n",
    "    params['d_hid'] = 8\n",
    "    params['batch_size'] = 5\n",
    "    testnet = ATTNLM(params)\n",
    "    testnet.to(device)\n",
    "    test_output = testnet(test_batch)\n",
    "    assert test_output.shape[0] == params['batch_size'], \"size of dimension 0 is incorrect, expect %i but got %i\" % \\\n",
    "                                                          (params['batch_size'], test_output.shape[0])\n",
    "    assert test_output.shape[1] == test_batch.shape[1], \"size of dimension 1 is incorrect, expect %i but got %i\" % \\\n",
    "                                                          (test_batch.shape[1], test_output.shape[1])\n",
    "    assert test_output.shape[2] == params['vocab_size'], \"size of dimension 2 is incorrect, expect %i but got %i\" % \\\n",
    "                                                          (params['vocab_size'], test_output.shape[2])\n",
    "    testnet = ATTNLM(params)\n",
    "    testnet.to(device)\n",
    "    test_output = testnet(test_batch, return_attn_weights=True)\n",
    "    assert test_output.shape[0] == params['batch_size'], \"size of dimension 0 is incorrect, expect %i but got %i\" % \\\n",
    "                                                          (params['batch_size'], test_output.shape[0])\n",
    "    assert test_output.shape[1] == test_batch.shape[1], \"size of dimension 1 is incorrect, expect %i but got %i\" % \\\n",
    "                                                          (test_batch.shape[1], test_output.shape[1])\n",
    "    assert test_output.shape[2] == test_batch.shape[1], \"size of dimension 2 is incorrect, expect %i but got %i\" % \\\n",
    "                                                          (test_batch.shape[1], test_output.shape[2])\n",
    "    prob_dist = torch.sum(test_output, dim=2)[:, 1:]\n",
    "    assert all([x > 0.99 and x < 1.01 for x in prob_dist.reshape(-1)]), \"attention weights not properly normalized, got {}\".format(prob_dist)\n",
    "    print(\"Congratulations, you passed the ATTNLM test!\")\n",
    "\n",
    "test_ATTNLM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRxeoJW3Ltzk"
   },
   "source": [
    "Now, train your `ATTNLM` model on WikiText by running the following code cell. If the perplexity on dev set is `nan` or `inf`, it is likely the model is corrupted due to gradient exploding/vanishing or other numerical instability issue; stop this cell and run it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81LKL_7pKYAC",
    "outputId": "62b43fab-6cce-4716-e8ea-ed3d006277e7"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THESE HYPERPARAMETERS, WE WILL CHECK!\n",
    "params = {}\n",
    "params['vocab_size'] = len(idx_to_word)\n",
    "params['d_emb'] = 512\n",
    "params['d_hid'] = 256\n",
    "params['n_layer'] = 1\n",
    "params['batch_size'] = 64\n",
    "params['epochs'] = 6\n",
    "params['learning_rate'] = 0.0005\n",
    "\n",
    "ATTNnet = ATTNLM(params)\n",
    "ATTNnet.cuda()\n",
    "train_lm(wikitext['train'], params, ATTNnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENa03ZlJLyFF"
   },
   "source": [
    "Finally, compute the perplexity on the test set. If you implemented it correctly, you should get a perplexity of around 145-150. Due to random effects, it is possible to get perplexity slightly lower than 145. Make sure you didn't add any additional nonlinearity operation which can lead to lower perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LoOvc6quW0Ef",
    "outputId": "9165c1ac-5da6-44d8-f04e-bc4c2922637a"
   },
   "outputs": [],
   "source": [
    "ATTNnet.eval() # we're no longer training the network\n",
    "print('%s perplexity: %0.2f' % ('test', compute_perplexity(wikitext['test'], ATTNnet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCyBN6KtEpLy"
   },
   "source": [
    "## Q2.4 Generate text from the neural LMs (5 pts)\n",
    "Run the following code cell to generate some text from your `RNNLM` and `ATTNLM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fkzzhEYCCY5A",
    "outputId": "6c9b08db-bab1-4f5d-ccdd-9c0e0508c564"
   },
   "outputs": [],
   "source": [
    "def sample_from_lm(net, context, max_words=50):\n",
    "  \n",
    "    with torch.no_grad():\n",
    "        for i in range(max_words):\n",
    "            data = torch.LongTensor([context]).to(device)\n",
    "            decoded = net(data)\n",
    "            decoded = decoded[0, -1].exp().cpu()\n",
    "            w_i = torch.multinomial(decoded, 1)[0].item()\n",
    "            if w_i in [1, 2, 3]:\n",
    "                continue\n",
    "            context.append(w_i)\n",
    "\n",
    "        return context\n",
    "\n",
    "word_to_idx = dict((v,k) for (k,v) in idx_to_word.items())\n",
    "context = [word_to_idx[w] for w in 'he is the '.split()]\n",
    "\n",
    "rnn_completion = sample_from_lm(RNNnet, context)\n",
    "print('rnn completion: ', ' '.join([idx_to_word[w] for w in rnn_completion]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QYg9ki0dYoyi",
    "outputId": "1fdd8549-048d-4260-ed2e-290c2c463139"
   },
   "outputs": [],
   "source": [
    "word_to_idx = dict((v,k) for (k,v) in idx_to_word.items())\n",
    "context = [word_to_idx[w] for w in 'he is the '.split()]\n",
    "\n",
    "rnn_completion = sample_from_lm(ATTNnet, context)\n",
    "print('attention rnn completion: ', ' '.join([idx_to_word[w] for w in rnn_completion]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KU63vLoNE76l"
   },
   "source": [
    "Do you notice any differences in coherence or grammaticality compared to the n-gram models? What about any differences between the `RNNLM` and the `ATTNLM`? If you observed any distinct differences, explain why you think they exist; if not, explain why all of the outputs appear to be of similar quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Krumh-KN3fw"
   },
   "source": [
    "### <font color=\"red\">*Answer in two to four sentences here*.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xBGJPkitfizQ"
   },
   "source": [
    "## Q2.5 Interpreting attention (5 pts)\n",
    "Finally, let's visualize some attention heatmaps by running the following two code cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "jXykMRedFfE2",
    "outputId": "0713e506-e5e3-46b9-ef75-b0c55fb984fd"
   },
   "outputs": [],
   "source": [
    "def plot_attn_heatmap(sent):\n",
    "  \n",
    "    sent_in_id = [word_to_idx[w] for w in sent.split()]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        data = torch.LongTensor([sent_in_id]).to(device)\n",
    "        weights = ATTNnet(data, return_attn_weights=True)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    sent_sp = sent.split()\n",
    "    ax.set_xticks(np.arange(len(sent_sp)))\n",
    "    ax.set_yticks(np.arange(len(sent_sp)))\n",
    "    ax.set_xticklabels(sent_sp)\n",
    "    ax.set_yticklabels(sent_sp)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right', rotation_mode=\"anchor\")\n",
    "\n",
    "    plt.imshow(weights[0, :].cpu())\n",
    "\n",
    "sent = \"top warning signs earth is warming , according to experts\"\n",
    "plot_attn_heatmap(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290
    },
    "id": "szDBrOdpFflZ",
    "outputId": "96f09d9e-10d9-4598-f298-7ceb336b7a3c"
   },
   "outputs": [],
   "source": [
    "sent = \"us cities lose 36 million trees each year . here is why it matters \"\n",
    "plot_attn_heatmap(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGh2jq9tG50n"
   },
   "source": [
    "Each row of these plots represents the attention weights on the history tokens when the model is trying to predict the next word. For example, the third row of the first plot can be interpreted as the attention weights over \"top\" and \"warning\" when predicting \"signs\"; you'll note that the rest of the row is black (i.e., zero attention on future words). Are these attention maps interpretable? If you (as a human) were solving the same word prediction problem, would you focus on the same words as the ATTNLM does?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBfgLYgNN3fx"
   },
   "source": [
    "### <font color=\"red\">*Answer in two to four sentences here*.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer goes here."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
